{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "from time import perf_counter\n",
    "from sklearn import preprocessing\n",
    "from scipy.linalg import fractional_matrix_power, funm\n",
    "from scipy.io import arff\n",
    "from math import sqrt\n",
    "from numba import jit\n",
    "\n",
    "''' \n",
    "    Create a co-ocurrence matrix and map each word to an unique id.\n",
    "    Input: Matrix of documents by row\n",
    "    Output: An co-ocurrence matrix and a dict{word:id}\n",
    "'''\n",
    "def create_co_occurences_matrix(X, words):\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "    X[X>0] = 1\n",
    "    return csr_matrix(X.T.dot(X)), word_to_id\n",
    "\n",
    "def load_from_arff(filename, class_column_name):\n",
    "    data, meta = arff.loadarff(open(filename, 'r'))\n",
    "\n",
    "    X = pd.DataFrame(data)\n",
    "    y = X[class_column_name]\n",
    "    \n",
    "    X.drop(columns=[class_column_name], inplace=True)\n",
    "    X.sort_index(axis=1, inplace=True)\n",
    "    \n",
    "    return X.values, y, sorted(meta.names()[:-1])\n",
    "\n",
    "def split_balanced(data, target, test_size=0.2):\n",
    "    classes = np.unique(target)\n",
    "    # can give test_size as fraction of input data size of number of samples\n",
    "    if test_size<1:\n",
    "        n_test = np.round(len(target)*test_size)\n",
    "    else:\n",
    "        n_test = test_size\n",
    "    n_train = max(0,len(target)-n_test)\n",
    "    n_train_per_class = max(1,int(np.floor(n_train/len(classes))))\n",
    "    n_test_per_class = max(1,int(np.floor(n_test/len(classes))))\n",
    "\n",
    "    ixs = []\n",
    "    for cl in classes:\n",
    "        if (n_train_per_class+n_test_per_class) > np.sum(target==cl):\n",
    "            # if data has too few samples for this class, do upsampling\n",
    "            # split the data to training and testing before sampling so data points won't be\n",
    "            #  shared among training and test data\n",
    "            splitix = int(np.ceil(n_train_per_class/(n_train_per_class+n_test_per_class)*np.sum(target==cl)))\n",
    "            ixs.append(np.r_[np.random.choice(np.nonzero(target==cl)[0][:splitix], n_train_per_class),\n",
    "                np.random.choice(np.nonzero(target==cl)[0][splitix:], n_test_per_class)])\n",
    "        else:\n",
    "            ixs.append(np.random.choice(np.nonzero(target==cl)[0], n_train_per_class+n_test_per_class,\n",
    "                replace=False))\n",
    "\n",
    "    # take same num of samples from all classes\n",
    "    ix_train = np.concatenate([x[:n_train_per_class] for x in ixs])\n",
    "    ix_test = np.concatenate([x[n_train_per_class:(n_train_per_class+n_test_per_class)] for x in ixs])\n",
    "\n",
    "    X_train = data[ix_train,:]\n",
    "    X_test = data[ix_test,:]\n",
    "    y_train = target[ix_train]\n",
    "    y_test = target[ix_test]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288 8 [0 0 1 1 2 2 3 3]\n",
      "Execution time:  0.3634406229999998\n"
     ]
    }
   ],
   "source": [
    "start_execution = perf_counter()\n",
    "\n",
    "X, y, words = load_from_arff('../data/CSTR.arff', 'class_atr')\n",
    "y, levels = pd.factorize(y)\n",
    "\n",
    "total_docs = len(X)\n",
    "\n",
    "#df = load_from_csv('small_news_.csv')\n",
    "#X = pre_process(df['0'].values)\n",
    "#y = df['class']\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=len(X) - len(np.unique(y)), random_state=42, stratify=y)\n",
    "X_train, X_test, y_train, y_test = split_balanced(X, y, test_size=len(X) - (2*len(np.unique(y))))\n",
    "\n",
    "X = csr_matrix(np.concatenate((X_train,X_test)))\n",
    "y = np.concatenate((y_train,y_test))\n",
    "\n",
    "print(len(y_test), len(y_train), y_train)\n",
    "\n",
    "words_cooc_matrix, word_to_id = create_co_occurences_matrix(X, words)\n",
    "\n",
    "n_labeled = len(y_train)\n",
    "\n",
    "# Binarize labels in a one-vs-all fashion\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(y_train)\n",
    "Y_bin = lb.transform(y_train)\n",
    "Y_input = np.concatenate((Y_bin, np.zeros((y.size-Y_bin.shape[0], Y_bin.shape[1]))))\n",
    "\n",
    "print(\"Execution time: \", perf_counter() - start_execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Construct a contingency matrix\n",
    "'''\n",
    "@jit\n",
    "def full_contingency_matrix(cooc_matrix, total_docs):\n",
    "    p_11 = (cooc_matrix/total_docs)\n",
    "    p_10 = (csr_matrix.diagonal(cooc_matrix)[:, np.newaxis] - cooc_matrix)/total_docs\n",
    "    p_01 = (csr_matrix.diagonal(cooc_matrix) - cooc_matrix)/total_docs\n",
    "    p_00 = 1-(p_11+p_10+p_01)\n",
    "    \n",
    "    return np.array([p_11, p_10, p_01, p_00])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full contingency matrix time:  0.30579117700000014\n"
     ]
    }
   ],
   "source": [
    "start_execution = perf_counter()\n",
    "A = full_contingency_matrix(words_cooc_matrix, total_docs)\n",
    "print(\"Full contingency matrix time: \", perf_counter() - start_execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.680129333\n",
      "Min -0.9817405801123438 Max 1.0\n",
      "0.8821419449999999\n"
     ]
    }
   ],
   "source": [
    "#from scipy.sparse.csr_matrix import diagonal\n",
    "'''\n",
    "    Construct a term network matrix\n",
    "    Input:\n",
    "        cooc_matrix: Co-ocurrence matrix;\n",
    "        words_id: dictionary of each word mapped to an unique id;\n",
    "        total_docs: Total of documents;\n",
    "        sim_func: similarity function.\n",
    "    Output: Term Network\n",
    "'''\n",
    "def construct_term_network(cooc_matrix, cm, words_id, total_docs):\n",
    "    #W = lil_matrix((len(words_id), len(words_id)), dtype=np.float_)\n",
    "    #W = np.zeros((len(words_id), len(words_id)))\n",
    "    #cm[0] - (np.diag(c[1])*np.diag()\n",
    "    return cm[0].A\n",
    "\n",
    "@jit             \n",
    "def support(cm):\n",
    "    return cm[0].A\n",
    "\n",
    "@jit             \n",
    "def piatetsky_shapiro(cm):\n",
    "    p_11 = cm[0]\n",
    "    return (p_11 - (p_11.diagonal()*p_11.diagonal()[:,np.newaxis]))\n",
    "\n",
    "@jit\n",
    "def yule(cm):\n",
    "    p_11 = cm[0]\n",
    "    p_10 = cm[1]\n",
    "    p_01 = cm[2]\n",
    "    p_00 = cm[3]\n",
    "    \n",
    "    part1 = p_11*p_00\n",
    "    part2 = p_10*p_01\n",
    "    \n",
    "    numerator = (part1 - part2)\n",
    "    denominator = (part1 + part2)\n",
    "    \n",
    "    return np.divide(numerator,denominator,where=denominator!=0)\n",
    "\n",
    "@jit\n",
    "def mutual_information(cm):\n",
    "    p_11 = cm[0].A\n",
    "    p_10 = cm[1].A\n",
    "    p_01 = cm[2].A\n",
    "    p_00 = cm[3].A\n",
    "    \n",
    "    p_1_p_1 = (np.diag(p_11)*np.diag(p_11)[:,np.newaxis])\n",
    "    p_1_p_0 = (np.diag(p_00)*np.diag(p_11)[:,np.newaxis])\n",
    "    p_0_p_1 = (np.diag(p_11)*np.diag(p_00)[:,np.newaxis])\n",
    "    p_0_p_0 = (np.diag(p_00)*np.diag(p_00)[:,np.newaxis])\n",
    "    '''\n",
    "    p_1_p_1 = (p_11.diagonal()*p_11.diagonal()[:,np.newaxis])\n",
    "    p_1_p_0 = (p_00.diagonal()*p_11.diagonal()[:,np.newaxis])\n",
    "    p_0_p_1 = (p_11.diagonal()*p_00.diagonal()[:,np.newaxis].T)\n",
    "    p_0_p_0 = (p_00.diagonal()*p_00.diagonal()[:,np.newaxis].T)\n",
    "    '''\n",
    "    p1 = np.divide(p_11,p_1_p_1,where=p_1_p_1!=0)\n",
    "    p2 = np.divide(p_10,p_1_p_0,where=p_1_p_0!=0)\n",
    "    p3 = np.divide(p_01,p_0_p_1,where=p_0_p_1!=0)\n",
    "    p4 = np.divide(p_00,p_0_p_0,where=p_0_p_0!=0)\n",
    "    \n",
    "    return  p_11*np.log2(p1, where=p1>0) +\\\n",
    "            p_10*np.log2(p2, where=p2>0) +\\\n",
    "            p_01*np.log2(p3, where=p3>0) +\\\n",
    "            p_00*np.log2(p4, where=p4>0)\n",
    "\n",
    "@jit\n",
    "def kappa(cm):\n",
    "    p_11 = cm[0].A\n",
    "    p_00 = cm[3].A\n",
    "    \n",
    "    p_1_p_1 = (np.diag(p_11)*np.diag(p_11)[:,np.newaxis])\n",
    "    p_0_p_0 = (np.diag(p_00)*np.diag(p_00)[:,np.newaxis])\n",
    "    \n",
    "    aux = p_1_p_1 - p_0_p_0\n",
    "    \n",
    "    num = (p_11 + p_00 - aux)\n",
    "    den = 1 - (aux)\n",
    "    \n",
    "    return np.divide(num,den,where=den!=0)\n",
    "    \n",
    "@jit\n",
    "def calculate_s(W):\n",
    "    S = np.zeros_like(W)\n",
    "    d = np.sum(W, axis=1)\n",
    "\n",
    "    for i in range(W.shape[0]):\n",
    "        for j in range(W.shape[1]):\n",
    "            if d[i] == 0 or d[j] == 0:\n",
    "                S[i][j] = 0\n",
    "            else:\n",
    "                S[i][j] = W[i][j] / (sqrt(d[i]) * sqrt(d[j]))\n",
    "    return S\n",
    "\n",
    "@jit\n",
    "def fractional_power(W):\n",
    "    d = np.sum(W, axis=1)\n",
    "    D = np.sqrt(d*d[:, np.newaxis])\n",
    "    return np.divide(W,D,where=D!=0)\n",
    "\n",
    "start_execution = perf_counter()\n",
    "\n",
    "W = yule(A)#construct_term_network(words_cooc_matrix, A, word_to_id, total_docs)\n",
    "\n",
    "print(perf_counter() - start_execution)\n",
    "print(\"Min\", np.min(W), \"Max\", np.max(W))\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "W = min_max_scaler.fit_transform(W)\n",
    "S = fractional_power(W)\n",
    "print(perf_counter() - start_execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_scores_matrix(y, dt_matrix, words_id):\n",
    "    sum_freq_total = dt_matrix.sum(axis = 0)\n",
    "    F = []\n",
    "    \n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        for j in np.unique(y):\n",
    "            F.append(np.nan_to_num(np.squeeze(np.asarray(dt_matrix[np.where(y==j)].sum(axis = 0)/sum_freq_total)), copy=False))\n",
    "    \n",
    "    return np.array(F).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.001851659000010386\n"
     ]
    }
   ],
   "source": [
    "start_execution = perf_counter()\n",
    "F = initialize_scores_matrix(y_train, X_train, word_to_id)\n",
    "Y_input_terms = copy.deepcopy(F) # \n",
    "print(\"Execution time:\", perf_counter() - start_execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@jit\n",
    "import cython\n",
    "\n",
    "@cython.wraparound(False)\n",
    "@cython.boundscheck(False)\n",
    "def llgc(alpha, S, F, Y_input_terms, n_iter):\n",
    "    F_result = F\n",
    "    for _ in range(n_iter):\n",
    "        F_result = alpha*np.dot(S, F_result) + (1-alpha)*Y_input_terms\n",
    "    return F_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7312921270000001\n"
     ]
    }
   ],
   "source": [
    "n_iter = 400\n",
    "alpha=0.1\n",
    "\n",
    "start_execution = perf_counter()\n",
    "F = llgc(alpha, S, F, Y_input_terms, n_iter)\n",
    "print(perf_counter() - start_execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(F):\n",
    "    Y_result = np.zeros_like(F)\n",
    "    Y_result[np.arange(len(F)), F.argmax(1)] = 1\n",
    "    return Y_result\n",
    "\n",
    "def classify_docs(X, F):\n",
    "    return X.dot(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of documents correctly classified: 114 0.38513513513513514\n"
     ]
    }
   ],
   "source": [
    "F_docs = classify_docs(X,F)\n",
    "y_answer = classify(F_docs)\n",
    "y_v = lb.inverse_transform(y_answer)\n",
    "\n",
    "print('Total of documents correctly classified:',sum(y_v==y), sum(y_v==y)/y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
